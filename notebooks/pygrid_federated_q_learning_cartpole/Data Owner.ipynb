{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6796fda",
   "metadata": {},
   "source": [
    "# Data Owner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6b6c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Before running this notebook, run the Data Scientist notebook to host a model in PyGrid.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf803f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from math import prod\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import gym\n",
    "import requests\n",
    "import torch as T\n",
    "from torch import nn\n",
    "from websocket import create_connection\n",
    "\n",
    "from syft import serialize\n",
    "from syft.federated.model_serialization import (\n",
    "    deserialize_model_params,\n",
    "    wrap_model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5883bf",
   "metadata": {},
   "source": [
    "## Step 1: Specify configuration and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e8e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"q-learning\"\n",
    "VERSION = \"0.0.0\"\n",
    "GRID_ADDRESS = \"localhost:5000\"\n",
    "CARTPOLE_DIMS = (1, 1, 12, 6, 2)\n",
    "\n",
    "# (cart position, cart velocity, pole angle, pole angular velocity)\n",
    "CartPoleObservation = tuple[float, float, float, float]\n",
    "DiscretizedCartPoleObservation = tuple[int, int, int, int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df3db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify(json_data: object) -> str:\n",
    "    return json.dumps(json_data, indent=2).replace(\"\\\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f5c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params(model: nn.Module, params: list[T.Tensor]) -> None:\n",
    "    for p, p_new in zip(model.parameters(), params):\n",
    "        p.data = p_new.detach().clone().data\n",
    "\n",
    "\n",
    "def calculate_diff(\n",
    "    original_params: list[T.Tensor], trained_params: list[T.Tensor]\n",
    ") -> list[T.Tensor]:\n",
    "    return [old - new for old, new in zip(original_params, trained_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82964c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_ws_message(grid_address: str, data: object) -> Any:\n",
    "    ws = create_connection(\"ws://\" + grid_address)\n",
    "    ws.send(json.dumps(data))\n",
    "    message = ws.recv()\n",
    "    return json.loads(message)\n",
    "\n",
    "\n",
    "def get_model_params(\n",
    "    grid_address: str, worker_id: str, request_key: str, model_id: str\n",
    ") -> list[T.Tensor]:\n",
    "    get_params = {\n",
    "        \"worker_id\": worker_id,\n",
    "        \"request_key\": request_key,\n",
    "        \"model_id\": model_id,\n",
    "    }\n",
    "    response = requests.get(\n",
    "        f\"http://{grid_address}/model-centric/get-model\", get_params\n",
    "    )\n",
    "    return deserialize_model_params(response.content)\n",
    "\n",
    "\n",
    "def retrieve_model_params(grid_address: str, name: str, version: str) -> list[T.Tensor]:\n",
    "    get_params = {\n",
    "        \"name\": name,\n",
    "        \"version\": version,\n",
    "        \"checkpoint\": \"latest\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(\n",
    "        f\"http://{grid_address}/model-centric/retrieve-model\", get_params\n",
    "    )\n",
    "    return deserialize_model_params(response.content)\n",
    "\n",
    "\n",
    "def send_auth_request(grid_address: str, name: str, version: str) -> Any:\n",
    "    message = {\n",
    "        \"type\": \"model-centric/authenticate\",\n",
    "        \"data\": {\n",
    "            \"model_name\": name,\n",
    "            \"model_version\": version,\n",
    "        },\n",
    "    }\n",
    "    return send_ws_message(grid_address, message)\n",
    "\n",
    "\n",
    "def send_cycle_request(\n",
    "    grid_address: str, name: str, version: str, worker_id: str\n",
    ") -> Any:\n",
    "    message = {\n",
    "        \"type\": \"model-centric/cycle-request\",\n",
    "        \"data\": {\n",
    "            \"worker_id\": worker_id,\n",
    "            \"model\": name,\n",
    "            \"version\": version,\n",
    "            \"ping\": 1,\n",
    "            \"download\": 10000,\n",
    "            \"upload\": 10000,\n",
    "        },\n",
    "    }\n",
    "    return send_ws_message(grid_address, message)\n",
    "\n",
    "\n",
    "def send_diff_report(\n",
    "    grid_address: str, worker_id: str, request_key: str, diff: list[T.Tensor]\n",
    ") -> Any:\n",
    "    serialized_diff = serialize(wrap_model_params(diff)).SerializeToString()\n",
    "    message = {\n",
    "        \"type\": \"model-centric/report\",\n",
    "        \"data\": {\n",
    "            \"worker_id\": worker_id,\n",
    "            \"request_key\": request_key,\n",
    "            \"diff\": base64.b64encode(serialized_diff).decode(\"ascii\"),\n",
    "        },\n",
    "    }\n",
    "    send_ws_message(grid_address, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392ef46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(min_value, max_value, value):\n",
    "    return max(min_value, min(max_value, value))\n",
    "\n",
    "def discretize(observation: CartPoleObservation) -> DiscretizedCartPoleObservation:\n",
    "    (\n",
    "        _raw_cart_position,\n",
    "        _raw_cart_velocity,\n",
    "        raw_pole_angle,\n",
    "        raw_pole_angular_velocity,\n",
    "    ) = observation\n",
    "    cart_position = 0  # not very useful\n",
    "    cart_velocity = 0  # not very useful\n",
    "    pole_angle = int(clip(0.0, 0.417, raw_pole_angle + 0.209) // (0.418 / 12))\n",
    "    pole_angular_velocity = int(\n",
    "        clip(0.0, 5.999, raw_pole_angular_velocity + 3.0) // (6.0 / 6)\n",
    "    )\n",
    "    return (cart_position, cart_velocity, pole_angle, pole_angular_velocity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860247d5",
   "metadata": {},
   "source": [
    "## Step 2: Define the model and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c41735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(nn.Module):\n",
    "    def __init__(self, alpha: float, gamma: float, min_epsilon: float, epsilon_reduction: float) -> None:\n",
    "        super().__init__()\n",
    "        self.name = \"q-learning\"\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_reduction = epsilon_reduction  # per action\n",
    "        self.network = nn.Linear(prod(CARTPOLE_DIMS), 1, bias=True)\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    def get_epsilon(self, train: bool) -> float:\n",
    "        if train:\n",
    "            epsilon = self.network.bias.item()\n",
    "            new_epsilon = max(self.min_epsilon, epsilon - self.epsilon_reduction)\n",
    "            self.network.bias.data = T.tensor([new_epsilon], requires_grad=False).data\n",
    "\n",
    "        return self.network.bias.item()\n",
    "\n",
    "    def get_q_values_for_observation(\n",
    "        self, observation: CartPoleObservation\n",
    "    ) -> T.Tensor:\n",
    "        q_table = self.network.weight.reshape(CARTPOLE_DIMS)\n",
    "        cart_position, cart_velocity, pole_angle, pole_angular_velocity = discretize(observation)\n",
    "        return q_table[cart_position][cart_velocity][pole_angle][pole_angular_velocity]\n",
    "\n",
    "    def act(self, observation: CartPoleObservation, train: bool) -> int:\n",
    "        if random.random() < self.get_epsilon(train):\n",
    "            return random.randrange(2)\n",
    "\n",
    "        return int(self.get_q_values_for_observation(observation).argmax())\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        observation: CartPoleObservation,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        observation_next: CartPoleObservation,\n",
    "    ) -> None:\n",
    "        q_values = self.get_q_values_for_observation(observation)\n",
    "        max_next_q_value = self.get_q_values_for_observation(observation_next).max()\n",
    "        q_values[action] = q_values[action] + self.alpha * (\n",
    "            reward + self.gamma * max_next_q_value - q_values[action]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69849bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iteration(agent: QLearningAgent, environment: gym.Env, train: bool) -> float:\n",
    "    ret = 0.0\n",
    "    observation = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(observation, train)\n",
    "        observation_next, reward, done, _ = environment.step(action)\n",
    "        ret += reward\n",
    "        if train:\n",
    "            agent.update(observation, action, reward, observation_next)\n",
    "        observation = observation_next\n",
    "\n",
    "    return ret\n",
    "\n",
    "def run_epoch(n_iterations: int, agent: QLearningAgent, train=True, period=100):\n",
    "    environment = gym.make(\"CartPole-v1\")\n",
    "    rets = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        ret = run_iteration(agent, environment, train)\n",
    "        rets.append(ret)\n",
    "        if (i + 1) % period == 0:\n",
    "            print(\n",
    "                f\"[federated {agent.name} agent] Epoch {i + 1} Average return per game: \"\n",
    "                + f\"{sum(rets[-period:]) / period} from {period} games\"\n",
    "            )\n",
    "\n",
    "    return list(agent.parameters()), rets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f45be0",
   "metadata": {},
   "source": [
    "## Step 3: Authenticate for cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57b001e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auth_response = send_auth_request(GRID_ADDRESS, NAME, VERSION)\n",
    "worker_id = auth_response[\"data\"][\"worker_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c5a95",
   "metadata": {},
   "source": [
    "## Step 4: Make cycle request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2014a035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cycle_response = send_cycle_request(GRID_ADDRESS, NAME, VERSION, worker_id)\n",
    "request_key = cycle_response[\"data\"][\"request_key\"]\n",
    "model_id = cycle_response[\"data\"][\"model_id\"]\n",
    "client_config = cycle_response[\"data\"][\"client_config\"]\n",
    "alpha = client_config[\"alpha\"]\n",
    "gamma = client_config[\"gamma\"]\n",
    "min_epsilon = client_config[\"min_epsilon\"]\n",
    "epsilon_reduction = client_config[\"epsilon_reduction\"]\n",
    "n_train_iterations = client_config[\"n_train_iterations\"]\n",
    "n_test_iterations = client_config[\"n_test_iterations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325b18a",
   "metadata": {},
   "source": [
    "## Step 5: Download the model parameters and set local model parameters accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea984505",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_params = get_model_params(GRID_ADDRESS, worker_id, request_key, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5addf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_agent = QLearningAgent(\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    min_epsilon=min_epsilon,\n",
    "    epsilon_reduction=epsilon_reduction,\n",
    ")\n",
    "set_params(local_agent, downloaded_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b035af",
   "metadata": {},
   "source": [
    "## Step 6: Train the local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96b7db69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[federated q-learning agent] Epoch 100 Average return per game: 136.9 from 100 games\n",
      "Pre-training performance: 136.9\n"
     ]
    }
   ],
   "source": [
    "_, pre_rets = run_epoch(n_test_iterations, local_agent, train=False)\n",
    "print(f\"Pre-training performance: {sum(pre_rets) / n_test_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61aa415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[federated q-learning agent] Epoch 100 Average return per game: 181.49 from 100 games\n",
      "[federated q-learning agent] Epoch 200 Average return per game: 181.41 from 100 games\n",
      "[federated q-learning agent] Epoch 300 Average return per game: 196.04 from 100 games\n"
     ]
    }
   ],
   "source": [
    "trained_params, _ = run_epoch(300, local_agent, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21ed8342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[federated q-learning agent] Epoch 100 Average return per game: 180.94 from 100 games\n",
      "Post-training performance: 180.94\n"
     ]
    }
   ],
   "source": [
    "_, post_rets = run_epoch(n_test_iterations, local_agent, train=False)\n",
    "print(f\"Post-training performance: {sum(post_rets) / n_test_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b19073",
   "metadata": {},
   "source": [
    "## Step 7: Calculate and send back the diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b1757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = calculate_diff(downloaded_params, trained_params)\n",
    "send_diff_report(GRID_ADDRESS, worker_id, request_key, diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745c5d3",
   "metadata": {},
   "source": [
    "## Step 8: Test updated remote model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2e33aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[federated q-learning agent] Epoch 100 Average return per game: 185.21 from 100 games\n",
      "Updated model performance: 185.21\n"
     ]
    }
   ],
   "source": [
    "new_model_params = retrieve_model_params(GRID_ADDRESS, NAME, VERSION)\n",
    "set_params(local_agent, new_model_params)\n",
    "\n",
    "_, updated_rets = run_epoch(n_test_iterations, local_agent, train=False)\n",
    "print(f\"Updated model performance: {sum(updated_rets) / n_test_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb063b68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "You can re-run this notebook to simulate multiple data owners contributing to the federated learning process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877c10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

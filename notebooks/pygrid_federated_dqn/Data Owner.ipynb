{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6796fda",
   "metadata": {},
   "source": [
    "# Data Owner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6b6c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Before running this notebook, run the Data Scientist notebook to host a model in PyGrid.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf803f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import gym\n",
    "import requests\n",
    "import torch as T\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from websocket import create_connection\n",
    "\n",
    "from syft import serialize\n",
    "from syft.federated.model_serialization import (\n",
    "    deserialize_model_params,\n",
    "    wrap_model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5883bf",
   "metadata": {},
   "source": [
    "## Step 1: Specify configuration and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e8e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"dqn\"\n",
    "VERSION = \"0.0.0\"\n",
    "GRID_ADDRESS = \"localhost:5000\"\n",
    "INPUT_WIDTH = 4\n",
    "HIDDEN_WIDTH = 4\n",
    "OUTPUT_WIDTH = 2\n",
    "\n",
    "# (cart position, cart velocity, pole angle, pole angular velocity)\n",
    "CartPoleObservation = tuple[float, float, float, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df3db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify(json_data: object) -> str:\n",
    "    return json.dumps(json_data, indent=2).replace(\"\\\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f5c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params(model: nn.Module, params: list[T.Tensor]) -> None:\n",
    "    for p, p_new in zip(model.parameters(), params):\n",
    "        p.data = p_new.detach().clone().data\n",
    "\n",
    "\n",
    "def calculate_diff(\n",
    "    original_params: list[T.Tensor], trained_params: list[T.Tensor]\n",
    ") -> list[T.Tensor]:\n",
    "    return [old - new for old, new in zip(original_params, trained_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82964c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_ws_message(grid_address: str, data: object) -> Any:\n",
    "    ws = create_connection(\"ws://\" + grid_address)\n",
    "    ws.send(json.dumps(data))\n",
    "    message = ws.recv()\n",
    "    return json.loads(message)\n",
    "\n",
    "\n",
    "def get_model_params(\n",
    "    grid_address: str, worker_id: str, request_key: str, model_id: str\n",
    ") -> list[T.Tensor]:\n",
    "    get_params = {\n",
    "        \"worker_id\": worker_id,\n",
    "        \"request_key\": request_key,\n",
    "        \"model_id\": model_id,\n",
    "    }\n",
    "    response = requests.get(\n",
    "        f\"http://{grid_address}/model-centric/get-model\", get_params\n",
    "    )\n",
    "    return deserialize_model_params(response.content)\n",
    "\n",
    "\n",
    "def retrieve_model_params(grid_address: str, name: str, version: str) -> list[T.Tensor]:\n",
    "    get_params = {\n",
    "        \"name\": name,\n",
    "        \"version\": version,\n",
    "        \"checkpoint\": \"latest\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(\n",
    "        f\"http://{grid_address}/model-centric/retrieve-model\", get_params\n",
    "    )\n",
    "    return deserialize_model_params(response.content)\n",
    "\n",
    "\n",
    "def send_auth_request(grid_address: str, name: str, version: str) -> Any:\n",
    "    message = {\n",
    "        \"type\": \"model-centric/authenticate\",\n",
    "        \"data\": {\n",
    "            \"model_name\": name,\n",
    "            \"model_version\": version,\n",
    "        },\n",
    "    }\n",
    "    return send_ws_message(grid_address, message)\n",
    "\n",
    "\n",
    "def send_cycle_request(\n",
    "    grid_address: str, name: str, version: str, worker_id: str\n",
    ") -> Any:\n",
    "    message = {\n",
    "        \"type\": \"model-centric/cycle-request\",\n",
    "        \"data\": {\n",
    "            \"worker_id\": worker_id,\n",
    "            \"model\": name,\n",
    "            \"version\": version,\n",
    "            \"ping\": 1,\n",
    "            \"download\": 10000,\n",
    "            \"upload\": 10000,\n",
    "        },\n",
    "    }\n",
    "    return send_ws_message(grid_address, message)\n",
    "\n",
    "\n",
    "def send_diff_report(\n",
    "    grid_address: str, worker_id: str, request_key: str, diff: list[T.Tensor]\n",
    ") -> Any:\n",
    "    serialized_diff = serialize(wrap_model_params(diff)).SerializeToString()\n",
    "    message = {\n",
    "        \"type\": \"model-centric/report\",\n",
    "        \"data\": {\n",
    "            \"worker_id\": worker_id,\n",
    "            \"request_key\": request_key,\n",
    "            \"diff\": base64.b64encode(serialized_diff).decode(\"ascii\"),\n",
    "        },\n",
    "    }\n",
    "    send_ws_message(grid_address, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860247d5",
   "metadata": {},
   "source": [
    "## Step 2: Define the model and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c41735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_width: int,\n",
    "        output_width: int,\n",
    "        hidden_width: int,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "        min_epsilon: float,\n",
    "        epsilon_reduction: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.name = NAME\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_reduction = epsilon_reduction  # per action\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_width, hidden_width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_width, output_width),\n",
    "        )\n",
    "        self.optimizer = optim.SGD(self.network.parameters(), lr=alpha)\n",
    "        self.epsilon = nn.Linear(1, 1, bias=False)\n",
    "        for p in self.epsilon.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    def get_epsilon(self, train: bool) -> float:\n",
    "        if train:\n",
    "            epsilon = self.epsilon.weight.item()\n",
    "            new_epsilon = max(self.min_epsilon, epsilon - self.epsilon_reduction)\n",
    "            self.epsilon.weight.data = T.tensor([new_epsilon], requires_grad=False).data\n",
    "\n",
    "        return self.epsilon.weight.item()\n",
    "\n",
    "    def act(self, observation: CartPoleObservation, train: bool) -> int:\n",
    "        if random.random() < self.get_epsilon(train):\n",
    "            return random.randrange(2)\n",
    "\n",
    "        processed_observation = T.tensor([observation], dtype=T.float32)\n",
    "        return self.network(processed_observation).argmax().item()\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        observation: CartPoleObservation,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        observation_next: CartPoleObservation,\n",
    "    ) -> None:\n",
    "        self.optimizer.zero_grad()\n",
    "        processed_observation = T.tensor([observation], dtype=T.float32)\n",
    "        processed_observation_next = T.tensor([observation_next], dtype=T.float32)\n",
    "        \n",
    "        current_q_value = self.network(processed_observation).squeeze()[action]\n",
    "        next_q_value = self.network(processed_observation_next).squeeze().max()\n",
    "        target_q_value = next_q_value * self.gamma + reward\n",
    "        \n",
    "        loss = F.mse_loss(current_q_value, target_q_value)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69849bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iteration(agent: QLearningAgent, environment: gym.Env, train: bool) -> float:\n",
    "    ret = 0.0\n",
    "    observation = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(observation, train)\n",
    "        observation_next, reward, done, _ = environment.step(action)\n",
    "        ret += reward\n",
    "        if train:\n",
    "            agent.update(observation, action, reward, observation_next)\n",
    "        observation = observation_next\n",
    "\n",
    "    return ret\n",
    "\n",
    "def run_epoch(n_iterations: int, agent: QLearningAgent, train=True, period=100):\n",
    "    environment = gym.make(\"CartPole-v1\")\n",
    "    rets = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        ret = run_iteration(agent, environment, train)\n",
    "        rets.append(ret)\n",
    "        if (i + 1) % period == 0:\n",
    "            print(\n",
    "                f\"[federated {agent.name} agent] Epoch {i + 1} Average return per game: \"\n",
    "                + f\"{sum(rets[-period:]) / period} from {period} games\"\n",
    "            )\n",
    "\n",
    "    return list(agent.parameters()), rets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f45be0",
   "metadata": {},
   "source": [
    "## Step 3: Authenticate for cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f57b001e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auth_response = send_auth_request(GRID_ADDRESS, NAME, VERSION)\n",
    "worker_id = auth_response[\"data\"][\"worker_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c5a95",
   "metadata": {},
   "source": [
    "## Step 4: Make cycle request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2014a035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cycle_response = send_cycle_request(GRID_ADDRESS, NAME, VERSION, worker_id)\n",
    "request_key = cycle_response[\"data\"][\"request_key\"]\n",
    "model_id = cycle_response[\"data\"][\"model_id\"]\n",
    "client_config = cycle_response[\"data\"][\"client_config\"]\n",
    "alpha = client_config[\"alpha\"]\n",
    "gamma = client_config[\"gamma\"]\n",
    "min_epsilon = client_config[\"min_epsilon\"]\n",
    "epsilon_reduction = client_config[\"epsilon_reduction\"]\n",
    "n_train_iterations = client_config[\"n_train_iterations\"]\n",
    "n_test_iterations = client_config[\"n_test_iterations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325b18a",
   "metadata": {},
   "source": [
    "## Step 5: Download the model parameters and set local model parameters accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea984505",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_params = get_model_params(GRID_ADDRESS, worker_id, request_key, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb5addf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_agent = QLearningAgent(\n",
    "    input_width=INPUT_WIDTH,\n",
    "    output_width=OUTPUT_WIDTH,\n",
    "    hidden_width=HIDDEN_WIDTH,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    min_epsilon=min_epsilon,\n",
    "    epsilon_reduction=epsilon_reduction,\n",
    ")\n",
    "set_params(local_agent, downloaded_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b035af",
   "metadata": {},
   "source": [
    "## Step 6: Train the local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b7db69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[federated dqn agent] Epoch 100 Average return per game: 50.11 from 100 games\n",
      "Pre-training performance: 50.11\n",
      "Epsilon: 0.009999999776482582\n"
     ]
    }
   ],
   "source": [
    "_, pre_rets = run_epoch(n_test_iterations, local_agent, train=False)\n",
    "print(f\"Pre-training performance: {sum(pre_rets) / n_test_iterations}\")\n",
    "print(f\"Epsilon: {local_agent.epsilon.weight.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61aa415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[federated dqn agent] Epoch 100 Average return per game: 58.83 from 100 games\n",
      "[federated dqn agent] Epoch 200 Average return per game: 51.15 from 100 games\n",
      "[federated dqn agent] Epoch 300 Average return per game: 52.02 from 100 games\n"
     ]
    }
   ],
   "source": [
    "trained_params, _ = run_epoch(300, local_agent, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21ed8342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[federated dqn agent] Epoch 100 Average return per game: 53.08 from 100 games\n",
      "Post-training performance: 53.08\n",
      "Epsilon: 0.009999999776482582\n"
     ]
    }
   ],
   "source": [
    "_, post_rets = run_epoch(n_test_iterations, local_agent, train=False)\n",
    "print(f\"Post-training performance: {sum(post_rets) / n_test_iterations}\")\n",
    "print(f\"Epsilon: {local_agent.epsilon.weight.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b19073",
   "metadata": {},
   "source": [
    "## Step 7: Calculate and send back the diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b1757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = calculate_diff(downloaded_params, trained_params)\n",
    "send_diff_report(GRID_ADDRESS, worker_id, request_key, diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745c5d3",
   "metadata": {},
   "source": [
    "## Step 8: Test updated remote model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2e33aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[federated dqn agent] Epoch 100 Average return per game: 55.85 from 100 games\n",
      "Updated model performance: 55.85\n"
     ]
    }
   ],
   "source": [
    "new_model_params = retrieve_model_params(GRID_ADDRESS, NAME, VERSION)\n",
    "set_params(local_agent, new_model_params)\n",
    "\n",
    "_, updated_rets = run_epoch(n_test_iterations, local_agent, train=False)\n",
    "print(f\"Updated model performance: {sum(updated_rets) / n_test_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb063b68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "You can re-run this notebook to simulate multiple data owners contributing to the federated learning process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877c10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
